<PRD>
# Overview

The "Magic Button" is an application conceived for a hackathon. Its primary purpose is to assist users in visualizing their most profound dreams and aspirations, especially those they currently view as unattainable. It addresses the challenge users face when feeling stuck or overwhelmed by their ambitious goals by offering a straightforward and accessible initial step through guided visualization exercises. These exercises are inspired by Katathym Imaginative Psychotherapy (KIP).

The target audience is broadly defined as "anyone" who possesses a dream they wish to explore and move closer to realizing. The inherent value of the "Magic Button" lies in providing an easy-to-use tool that empowers individuals to take a concrete first step towards actualizing their dreams, irrespective of their previous experience with visualization methods or self-help techniques.

# Core Features

1.  **User Dream Input & Initial Guidance**
    * **What it does**: Displays introductory text designed to inspire the user and offers an input field where they can articulate their cherished dream or aspiration.
    * **Why it's important**: This feature sets the emotional and contextual stage for the visualization process. It also captures the user's specific goal, which is crucial for generating a tailored and effective visualization exercise.
    * **How it works**: The application will display the "Intro text" as detailed in the "VH Magic Button Draft.md". A multi-line text input area (e.g., leveraging the Shadcn/UI `<Textarea>` component from the starter project) will be provided for users to type in their dream. As per our discussion, no supplementary contextual assistance or examples beyond the initial intro text will be offered for this input.

2.  **"Make the magic happen" Button & Visualization Generation**
    * **What it does**: When this button is pressed, it initiates the process of generating a personalized visualization exercise for the user.
    * **Why it's important**: This is the central "magic" of the application, acting as the catalyst that transforms the user's expressed dream into a guided visualization experience.
    * **How it works**:
        * Upon clicking, the application will first attempt to determine the user's current geographical location using the browser's Geolocation API.
        * If the user's location cannot be obtained (e.g., permission denied, API unavailable), the phrase "in a faraway land" will be used as a substitute for the location aspect within the AI prompt, as specified in the "VH Magic Button Draft.md".
        * The frontend application will then make a request to a newly established, dedicated public backend API endpoint (e.g., `/api/magic-button/generate-visualization`).
        * This API endpoint will be responsible for constructing a specific prompt to be sent to the OpenAI API. The prompt will integrate the user's stated goal (from the input field) and their determined geographical location (or the "in a faraway land" fallback). The prompt structure is: "Provide me with a visualization exercise, drawing inspiration from Katathym Imaginative Psychotherapy (KIP), designed to help a client visualize their desired goal, particularly one they currently perceive as unattainable. Begin the visualization by incorporating the client's current place of residence or physical location {user’s location}. The desired goal is decribed below: \\n {user’s goal}".
        * The backend endpoint will call the OpenAI API utilizing the "gpt-4.1-mini" model. It will use default parameters, mirroring the logic of the existing `/app/api/ai/generate/text/route.ts` but will be configured as a public endpoint for this specific feature.
        * The AI-generated visualization instructions are then returned from the backend to the frontend client.

3.  **Display of AI-Generated Visualization and Supplementary Instructions**
    * **What it does**: Subsequent to the AI generating the visualization exercise, the application will display this generated text to the user. Concurrently, it will also present "Additional instructions" designed to prepare the user for engaging with the visualization experience.
    * **Why it's important**: This provides the user with the core content of the visualization and offers guidance on how to best approach and immerse themselves in the exercise.
    * **How it works**: The text generated by the OpenAI API (the visualization exercise itself) and the "Additional instructions" (as quoted in the "VH Magic Button Draft.md") will be rendered clearly on the page. The "Start visualization" button will also be made visible at this stage.

4.  **"Start visualization" Button & Text-to-Speech (TTS) Playback**
    * **What it does**: When pressed by the user, this button initiates the audio playback of the AI-generated visualization instructions.
    * **Why it's important**: This feature offers an auditory guided meditation-like experience, which can significantly enhance immersion and the effectiveness of the visualization.
    * **How it works**:
        * The application will utilize the browser's built-in Web Speech API (specifically, `SpeechSynthesis`) to perform the text-to-speech conversion.
        * The system will attempt to select and use a female English voice for the playback. If such a voice is not available on the user's browser or operating system, it will gracefully fall back to a default voice provided by the browser.
        * Each time the "Start visualization" button is pressed, the audio playback of the visualization instructions will commence from the beginning.
        * For the initial version, no other playback controls (such as pause, stop, or speed adjustment) will be implemented.

# User Experience

* **User Personas**: The primary user is "anyone" who has a dream or aspiration they wish to visualize and work towards. This includes individuals from all backgrounds, irrespective of their familiarity with self-help or mindfulness techniques.
* **Key User Flows**:
    1.  User lands on the application page and is greeted by the "Intro text" and an input field for their dream.
    2.  User types their dream or aspiration into the provided input field.
    3.  User clicks the "Make the magic happen" button.
    4.  The application processes the request: determines location (with fallback) and communicates with the AI service via the backend.
    5.  The AI-generated visualization text, the "Additional instructions," and the "Start visualization" button dynamically appear on the page. The initial input field and "Make the magic happen" button may be hidden or disabled to guide focus.
    6.  User reads the displayed instructions and, when they feel ready, clicks the "Start visualization" button.
    7.  The visualization instructions are read aloud using TTS. The user can click the "Start visualization" button again to restart the audio from the beginning.
* **UI/UX Considerations**:
    * The application will be designed as a single-page experience for simplicity and focus.
    * The overall design aesthetic will aim for a serene, minimalist, and encouraging atmosphere, creating an environment conducive to introspection and visualization.
    * It will utilize the Shadcn/UI component library and TailwindCSS for a clean, modern, and responsive interface.
    * Theming capabilities provided by `next-themes` in the starter project will be respected, ensuring a consistent experience with user preferences (light/dark mode).
    * Typography will be clear, legible, and unobtrusive to help users focus on their thoughts and the visualization content.

# Technical Architecture

* **System Components**:
    * **Frontend**: Next.js (App Router), React. UI will be built using Shadcn/UI components and styled with TailwindCSS. Client-side JavaScript will manage user input, browser Geolocation API for location detection, AJAX calls to the backend API, dynamic rendering of AI responses, and controlling the browser's Web Speech API for TTS.
    * **Backend API**: A new, public Next.js API route (e.g., `/api/magic-button/generate-visualization`) will be developed. This route will be responsible for receiving requests from the frontend, securely constructing prompts (including location/fallback logic), and interfacing with the OpenAI API.
    * **AI Service**: OpenAI API, specifically targeting the "gpt-4.1-mini" model for generating the visualization exercises.
* **Data Models**: For this initial version, adhering to privacy considerations, there will be no persistent data storage for the user's dream input or the AI-generated visualization instructions. The experience is designed to be ephemeral.
* **APIs and Integrations**:
    * **Internal API**: A new public Next.js API route (e.g., `/api/magic-button/generate-visualization`) will be created to handle the core logic of communicating with the OpenAI service.
    * **External APIs**:
        * OpenAI API: For generating the textual visualization exercises.
        * Browser Geolocation API: For attempting to determine the user's current location.
        * Browser Web Speech API (SpeechSynthesis): For providing text-to-speech playback of the visualizations.
* **Infrastructure Requirements**: Standard web hosting suitable for a Next.js application will be required (e.g., Vercel, as suggested by the deployment scripts in the starter project `llm.txt`). No database (Convex) setup is needed for *this specific feature's* data.
* **Authentication**: No user authentication (e.g., Clerk) will be required to access or use the core Magic Button feature, ensuring public accessibility.

# Development Roadmap

* **MVP Requirements (Hackathon Scope)**:
    1.  A functional user interface that displays:
        * The "Intro text" as specified in "VH Magic Button Draft.md".
        * An input field for the user to enter their dream.
        * The "Make the magic happen" button.
    2.  Client-side JavaScript to capture the user's dream input and implement the logic to attempt retrieval of the user's location via the Browser Geolocation API, including the "in a faraway land" fallback mechanism.
    3.  Creation and deployment of a new, public Next.js API route (e.g., `/api/magic-button/generate-visualization`) that:
        * Accepts the user's dream text and location data (or fallback indicator) as input.
        * Constructs the KIP-inspired prompt as defined.
        * Securely calls the OpenAI "gpt-4.1-mini" model.
        * Handles responses and potential errors from the OpenAI API.
        * Returns the AI-generated visualization text or an appropriate error to the client.
    4.  Frontend logic to enable the "Make the magic happen" button to call the new backend API endpoint.
    5.  Dynamic display of the AI-generated visualization text on the page upon successful API response.
    6.  Display of the "Additional instructions" text (from "VH Magic Button Draft.md") alongside the visualization.
    7.  Implementation and display of the "Start visualization" button, which, when clicked, triggers TTS playback of the generated text using the browser's Web Speech API. This includes attempting to use a female English voice with a graceful fallback to a browser default and ensuring the button restarts TTS from the beginning on each press.
    8.  Implementation of basic, user-friendly error handling:
        * For OpenAI API failures: Display a message (e.g., "Sorry, the magic isn't flowing right now. Please try again.") and an option to retry.
        * For TTS failures (e.g., no voices, API blocked): Fail silently (as the user can still read the text) or display a minimal, non-intrusive message indicating audio playback issues.
    9.  Ensure the entire user experience, including all text and interactions, is in English for this initial version.
* **Future Enhancements (Out of Scope for Initial MVP)**:
    * User accounts (e.g., via Clerk) to allow saving and revisiting visualizations.
    * Persistent storage of dreams or AI-generated visualizations (e.g., in Convex DB).
    * Advanced TTS playback controls (e.g., pause, resume, stop, voice speed adjustment, voice selection).
    * Support for multiple languages (internationalization/localization).
    * Mechanisms for users to provide feedback on the quality or effectiveness of visualizations.
    * Integration with other platform features if the Magic Button becomes part of a larger application (e.g., sharing, community features).

# Logical Dependency Chain

1.  **Basic UI Shell & Input**:
    * Develop the foundational page layout using Next.js and Shadcn/UI components.
    * Implement the static "Intro text" display.
    * Add the dream input field (`<Textarea>`) and the "Make the magic happen" button (`<Button>`).
    * Write client-side JavaScript to capture the dream input.
2.  **Location Service & Fallback**:
    * Implement client-side logic to use the Browser Geolocation API.
    * Include the "in a faraway land" fallback logic.
3.  **Public API Endpoint Creation**:
    * Define and create the new Next.js public API route (e.g., `/api/magic-button/generate-visualization`).
4.  **OpenAI Integration (Backend)**:
    * Within the new API route, implement the server-side logic for:
        * Receiving dream and location/fallback data from the client request.
        * Securely constructing the KIP-inspired prompt for the OpenAI API.
        * Calling the OpenAI "gpt-4.1-mini" model.
        * Handling responses and any errors from the OpenAI service.
        * Returning the generated visualization text or an error message to the client.
5.  **Frontend API Call & Dynamic Display**:
    * Implement the client-side JavaScript logic for the "Make the magic happen" button to:
        * Gather the dream input and location data.
        * Call the newly created backend API endpoint.
        * Process the response:
            * On success: Dynamically display the AI-generated visualization text, the "Additional instructions" text, and make the "Start visualization" button visible.
            * On failure: Display a user-friendly error message and provide a retry mechanism.
6.  **Text-to-Speech (TTS) Functionality**:
    * Implement the client-side logic for the "Start visualization" button to:
        * Use the Browser Web Speech API to read the displayed AI-generated text aloud.
        * Attempt to select a female English voice, with a graceful fallback to a browser-default voice if the preferred voice is not available.
        * Ensure that each click on the button restarts the speech playback from the beginning.
        * Implement basic error handling for TTS failures (e.g., silent failure or a non-intrusive notification).
7.  **Styling, Final UI Polish & Testing**:
    * Apply TailwindCSS styles consistently.
    * Refine the UI using Shadcn components to achieve the desired serene, minimalist, and encouraging atmosphere.
    * Conduct end-to-end testing of the user flow.

# Risks and Mitigations

* **Technical Challenges**:
    * **Browser Geolocation API Inconsistency/Permissions**:
        * *Risk*: Users may deny location permission, or the API might be inaccurate or unavailable in certain browsers or on specific devices. This could hinder the personalization of the visualization prompt.
        * *Mitigation*: A clear fallback mechanism is defined: if location is unavailable, the AI prompt will use the phrase "in a faraway land". This ensures the core functionality remains operational.
    * **Web Speech API Voice Availability/Quality & Browser Compatibility**:
        * *Risk*: The preferred female English voice may not be available across all user browsers and operating systems, and the quality of available voices can vary significantly. The TTS API itself might fail or be blocked in some browser environments.
        * *Mitigation*: The system will be designed to request the preferred voice but will gracefully fall back to a default voice provided by the browser if the specific request cannot be met. If TTS fails entirely, it will either fail silently (allowing users to still read the text) or display a minimal, non-intrusive message indicating that audio playback is currently unavailable.
    * **OpenAI API Reliability, Latency, and Rate Limits (especially in a Hackathon context)**:
        * *Risk*: The OpenAI API might experience downtime, return responses slowly, or enforce rate limits that could affect the user experience, particularly under the time constraints of a hackathon.
        * *Mitigation*: User-friendly error messages (e.g., "Sorry, the magic isn't flowing right now. Please try again.") will be implemented for API failures. An option to retry the action will be provided. For a hackathon setting, some latency in AI response is generally acceptable. The choice of the "gpt-4.1-mini" model aims to balance capability with cost-effectiveness and potentially faster response times compared to larger models.
* **Scope Management (Hackathon Context)**:
    * *Risk*: The temptation to add features beyond the core "Magic Button" concept (e.g., user accounts, saving visualizations, advanced TTS controls) during the hackathon, potentially jeopardizing the completion of the MVP.
    * *Mitigation*: A clear MVP scope has been defined (see "Development Roadmap"). Sticking to these requirements will be a priority. Features like user authentication, persistent data storage, and advanced controls are explicitly out of scope for this initial hackathon version.
* **Resource Constraints (Hackathon Context)**:
    * *Risk*: Limited time and potentially limited developer resources for implementation and testing.
    * *Mitigation*: Leveraging browser-native APIs (Geolocation, Web Speech) reduces external dependencies and integration efforts. The decision not to implement backend data storage for dream content or user accounts significantly simplifies the MVP. The starter project (`llm.txt`) provides a robust foundation with pre-configured UI components and API routing, accelerating development.

# Appendix

* **Introductory Text (from "VH Magic Button Draft.md")**:
    “Shhh... Can you hear it? The Magic Button has amplified your connection to your deepest aspirations. Now it's your turn to speak. What's that one dream you've secretly cherished, the one you barely dared to admit even to yourself? Whisper it to us. Paint a picture with your words. Imagine you're describing it to the universe, and the universe is listening intently, ready to conspire in your favor. No dream is too big, too bold, or too 'out there.' Let your imagination run wild”

* **Additional Instructions Text (from "VH Magic Button Draft.md")**:
    “And know this: there is a gentle current waiting, a soft breeze ready to transport you to a place where all you've just whispered is wonderfully, beautifully true. It's a realm woven from your own desires. When you feel that quiet readiness stirring within, simply close your eyes. Allow yourself to drift, to be carried away by the visualization instructions that will soon unfold, like a secret map revealing itself.
    Press the button below when your heart says 'now,' and let the journey begin.”

* **AI Prompt Structure (based on "VH Magic Button Draft.md")**:
    "Provide me with a visualization exercise, drawing inspiration from Katathym Imaginative Psychotherapy (KIP), designed to help a client visualize their desired goal, particularly one they currently perceive as unattainable. Begin the visualization by incorporating the client's current place of residence or physical location {user’s location}. The desired goal is decribed below: \\n {user’s goal}"
    * `{user’s location}` will be replaced by the determined location or "in a faraway land".
    * `{user’s goal}` will be replaced by the text input from the user.
</PRD>